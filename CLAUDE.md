# Analytics Service

## System Architecture Overview

**Workspace**: Archmap Test Platform
**Architecture Style**: Microservices with event-driven communication
**This Component's Role**: Event-sourced analytics service that consumes all Kafka events to build pre-aggregated metrics and exposes an admin-only REST API
**Component Type**: service
**Position in Flow**: Receives from: Order Service (kafka), Payment Service (kafka), API Gateway (http) ‚Üí Stores in: Analytics Database (postgresql)

**Related Components**:
  - ‚Üê **Order Service** (service) - kafka - Publishes order lifecycle events (order.created, order.confirmed, order.shipped, order.cancelled)
  - ‚Üê **Payment Service** (service) - kafka - Publishes payment events (payment.authorized, payment.failed)
  - ‚Üê **API Gateway** (gateway) - http - Proxies REST API requests with JWT validation and admin role enforcement
  - ‚Üí **Analytics Database** (database) - postgresql - Persists raw event log and pre-aggregated metrics
  - ‚Üí **Order Events Schemas** (library) - npm - Shared schemas for event validation, topic names, and consumer group constants

## Patterns

### System-Wide Patterns

- **Event-Driven Architecture**: Consume events from Kafka topics (`order.events`, `payment.events`). Validate against Zod schemas from `@florea-alex/order-events-schemas`. Propagate correlation IDs from Kafka message headers for distributed tracing.

- **CQRS Read Model**: This service is the read side ‚Äî it builds materialized views from the event stream. Pre-aggregate at write time so reads are fast. Zero HTTP calls to other services.

- **Repository Pattern for Database Access**: Encapsulate database operations in repository functions. Isolate SQL from business logic for testability.

- **API Response Format**: `{ success, data, error, timestamp }` ‚Äî matches all other services.

- **Environment Variables Naming**: `SCREAMING_SNAKE_CASE` with `DB_*` and `KAFKA_*` prefixes. Sensible defaults for local dev.

## Conventions

- **API Response Format**: All REST endpoints return `{ success: boolean, data: any, error: string | null, timestamp: ISO8601 }`.
- **Environment Variables**: `SCREAMING_SNAKE_CASE`. DB_HOST, KAFKA_BROKERS, LOG_LEVEL, etc.
- **Correlation IDs**: Extract from Kafka message headers, fall back to event.correlationId, generate UUID if none. Attach to all log entries.
- **Logging**: Winston with structured JSON. Fields: timestamp, level, message, service, correlationId.
- **Health Check**: `GET /health` returns DB and Kafka consumer status. Public (no admin guard).
- **Admin Guard**: All analytics endpoints (except /health) require `x-user-role: admin` header. Defense-in-depth ‚Äî Gateway also enforces.
- **Testing**: Jest + Supertest. Mock Kafka and database. Follow patterns from other services.

## Boundaries & Constraints

‚úÖ **Responsibilities**:
- Consume ALL events from both Kafka topics (order.events, payment.events)
- Validate incoming events against shared schemas
- Build and maintain pre-aggregated metrics
- Store raw events for replay capability
- Expose admin-only REST API for analytics queries
- Deduplicate events to prevent double-counting
- Gracefully shutdown: stop Kafka consumer ‚Üí close Express server ‚Üí close database pool

‚ùå **NOT Responsible For**:
- Publishing events to Kafka
- Making HTTP calls to other services (pure event consumer)
- User authentication (handled by Gateway)
- Modifying orders, products, or any other service's data

üö´ **Do NOT**:
- Make HTTP calls to Order Service, Product Service, or any other service
- Crash the consumer on individual message failures (log and skip)
- Double-count events on Kafka redelivery
- Expose analytics endpoints without admin role check
- Store sensitive PII data (only metadata and amounts)
- Auto-commit Kafka offsets before successful processing

---

*This file was auto-generated by Atelier. Update it as the component evolves.*
